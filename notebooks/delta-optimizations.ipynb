{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "871c442e",
   "metadata": {},
   "source": [
    "# **Delta Lake Z ORDER**\n",
    "\n",
    "* Data size - 3.71GB\n",
    "* Total Rows - 160M+\n",
    "* Total distinct id1, id2 columns - 100\n",
    "* Cardinality of id1, id2 (perfect not too low now too high) - 1.6M rows per id\n",
    "\n",
    "* x3: Delta table that was initially created\n",
    "* x4: Delta table after it's been optimized via small file compaction\n",
    "* x5: Delta table Z Ordered by id1\n",
    "* x6: Delta table Z Ordered by id1 and id2\n",
    "* x7: Delta table liquid clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dee71e2",
   "metadata": {},
   "source": [
    "| Version / Optimization         | Query A (id1) \\[sec] | Query B (id2) \\[sec] | Query C (id1 & id2) \\[sec] | File Distribution                | Implementation Time | Files Skipped |\n",
    "| ------------------------------ | -------------------- | -------------------- | -------------------------- | -------------------------------- | ------------------- | ------------- |\n",
    "| **Normal (v3)**                | 12.80                | 6.90                 | 6.17                       | 130 MB × 28, 60 MB × 4 (3.71 GB) | –                   | 0 / 32        |\n",
    "| **Optimize / Compaction (v4)** | 11.40                | 4.80                 | 5.72                       | 1 GB × 3, 780 MB × 1 (3.71 GB)   | 7m 36.8s            | 0 / 4         |\n",
    "| **Z-Order on id1 (v5)**        | 7.20                 | 12.70                | 2.17                       | 1.2 GB × 2, 1.3 GB × 1 (3.71 GB) | 11m 51.3s           | 2 / 3         |\n",
    "| **Z-Order on id1 & id2 (v6)**  | 4.15                 | 6.30                 | 1.28                       | 1.2 GB × 2, 1.3 GB × 1 (3.71 GB) | 15m 54.7s           | 1 / 3         |\n",
    "| **Liquid Clustering (id2)**    | 13.80                | 7.80                 | 4.60                       | 130 MB × 28, 60 MB × 4 (3.71 GB) | 5m 34.8s            | ?             |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5382c775",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop() # only run if you want to stop a running spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18fb9fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "import time\n",
    "\n",
    "import os\n",
    "import findspark\n",
    "import pyspark\n",
    "\n",
    "# # Set environment variables (local paths)\n",
    "# os.environ[\"JAVA_HOME\"] = \"D:/Programs/Java\"\n",
    "# os.environ[\"HADOOP_HOME\"] = \"D:/Programs/hadoop\"\n",
    "# os.environ[\"SPARK_HOME\"] = \"D:/Programs/spark/spark-3.5.6-bin-hadoop3\"  # Adjust if different\n",
    "\n",
    "# # Initialize findspark\n",
    "# findspark.init(\"D:/Programs/spark/spark-3.5.6-bin-hadoop3\")\n",
    "\n",
    "# Create Spark session\n",
    "from pyspark.sql.types import IntegerType\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "738703ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta import configure_spark_with_delta_pip\n",
    "from delta.tables import DeltaTable\n",
    "# import deltalake\n",
    "# import levi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1a6a2a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.37:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>DeltaLake Spark Session</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1246db85b70>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"DeltaLake Spark Session\") \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef19b776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delta_path = \"D:/HPE-PROJECT/deltalake/delta-lake/normal\"\n",
    "# optimized_delta_path = \"D:/HPE-PROJECT/deltalake/delta-lake/optimized\"\n",
    "# zorder_path = \"D:/HPE-PROJECT/deltalake/delta-lake/zorder\"\n",
    "# liquid_cluster_path = \"D:/HPE-PROJECT/deltalake/delta-lake/liquidcuster\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a37a88",
   "metadata": {},
   "source": [
    "## **Create Delta Lake**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "efcfacce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initially we do not want liquid clustering to be enabled so we have to set it to false\n",
    "spark.conf.set(\"spark.databricks.delta.clusteredTable.enableClusteringTablePreview\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee8d156",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_path = \"../data/delta-test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b477b690",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"../raw_data/N_1e7_K_1e2_single.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d91474b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = (\n",
    "#     spark.read.format(\"parquet\")\n",
    "#     .option(\"header\", True)\n",
    "#     .load(r\"D:\\Internship\\delta-lake\\delta_test2\\part-00000-1ec6ab41-0aab-49aa-b339-f04245dfac47-c000.snappy.parquet\")\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5aaf42fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------------+---+---+-----+---+---+---------+\n",
      "|  id1|  id2|         id3|id4|id5|  id6| v1| v2|       v3|\n",
      "+-----+-----+------------+---+---+-----+---+---+---------+\n",
      "|id019|id039|id0000056956| 54| 25|79908|  1|  6|57.799172|\n",
      "|id030|id026|id0000030380| 15| 59|42247|  3| 12| 7.312644|\n",
      "|id046|id009|id0000071155| 52| 26|62841|  4| 11|48.458201|\n",
      "|id091|id041|id0000005890| 14| 48|20662|  2|  3|78.210290|\n",
      "|id015|id051|id0000021869| 13|  6| 3601|  1|  6|70.187575|\n",
      "|id087|id011|id0000022994|  5| 98|54829|  3| 10|49.395266|\n",
      "|id090|id079|id0000092893|  2| 15|11476|  0|  7| 2.061937|\n",
      "|id023|id036|id0000052395| 56| 99|81319|  3| 14|61.664015|\n",
      "|id006|id052|id0000014135| 77| 87|67494|  1|  3|24.189885|\n",
      "|id012|id060|id0000063474| 23| 39|42573|  2|  0|98.224646|\n",
      "|id009|id050|id0000045735| 97| 83|24196|  0|  8|54.763220|\n",
      "|id094|id028|id0000012094| 37| 62| 9843|  0| 12|10.042739|\n",
      "|id003|id090|id0000079403| 25| 62|33113|  1| 13|84.784790|\n",
      "|id084|id026|id0000033763| 31| 50|93707|  2|  0|51.073113|\n",
      "|id017|id073|id0000071690| 45| 90|61742|  0| 14|19.423704|\n",
      "|id032|id073|id0000070287| 93| 23|59758|  0|  1|78.570562|\n",
      "|id074|id040|id0000013121| 58| 60|16697|  3|  6|50.735067|\n",
      "|id010|id007|id0000089126| 74| 74|15600|  3|  3|22.917749|\n",
      "|id023|id013|id0000085910| 89| 39|78473|  0|  2|34.706663|\n",
      "|id096|id090|id0000051400| 54| 17|72663|  3|  2|85.726583|\n",
      "+-----+-----+------------+---+---+-----+---+---+---------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "# df.show()\n",
    "df = spark.read.format(\"csv\").option(\"header\", True).load(csv_path)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e4afd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To cal. the size of the file formed then-\n",
    "# df.write.mode(\"overwrite\").parquet(\"temp_df_size\")\n",
    "# import os\n",
    "# size_bytes = sum(os.path.getsize(os.path.join(\"temp_df_size\", f)) for f in os.listdir(\"temp_df_size\"))\n",
    "# size_mb = size_bytes / (1024 * 1024)\n",
    "# print(f\"Estimated DataFrame size: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f7ccce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"delta\").save(delta_path)\n",
    "\n",
    "# 16 files of 238 mb \n",
    "#time 22.5s \n",
    "\n",
    "#with local[4]\n",
    "# 4 files 238 mb time 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aad6756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are appending data 3 times so that we have good amount of data\n",
    "# so 3 versions will be created\n",
    "for i in range(3):  # Change 5 to 6 for 6 times\n",
    "    df.write.format(\"delta\").mode(\"append\").save(delta_path)\n",
    "#37 secs 64 parquet created \n",
    "#wiht local 4 \n",
    "# 52 secs 16 files 952 mb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2441f498",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_table = DeltaTable.forPath(spark, delta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0295cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|version|           timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|      3|2025-08-13 10:11:...|  NULL|    NULL|    WRITE|{mode -> Append, ...|NULL|    NULL|     NULL|          2|  Serializable|         true|{numFiles -> 4, n...|        NULL|Apache-Spark/4.0....|\n",
      "|      2|2025-08-13 10:11:...|  NULL|    NULL|    WRITE|{mode -> Append, ...|NULL|    NULL|     NULL|          1|  Serializable|         true|{numFiles -> 4, n...|        NULL|Apache-Spark/4.0....|\n",
      "|      1|2025-08-13 10:11:...|  NULL|    NULL|    WRITE|{mode -> Append, ...|NULL|    NULL|     NULL|          0|  Serializable|         true|{numFiles -> 4, n...|        NULL|Apache-Spark/4.0....|\n",
      "|      0|2025-08-13 10:10:...|  NULL|    NULL|    WRITE|{mode -> ErrorIfE...|NULL|    NULL|     NULL|       NULL|  Serializable|         true|{numFiles -> 4, n...|        NULL|Apache-Spark/4.0....|\n",
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delta_table.history().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc712540",
   "metadata": {},
   "source": [
    "## **Compact small files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c340a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_table = DeltaTable.forPath(spark, delta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d131be2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[path: string, metrics: struct<numFilesAdded:bigint,numFilesRemoved:bigint,filesAdded:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,filesRemoved:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,partitionsOptimized:bigint,zOrderStats:struct<strategyName:string,inputCubeFiles:struct<num:bigint,size:bigint>,inputOtherFiles:struct<num:bigint,size:bigint>,inputNumCubes:bigint,mergedFiles:struct<num:bigint,size:bigint>,numOutputCubes:bigint,mergedNumCubes:bigint>,clusteringStats:struct<inputZCubeFiles:struct<numFiles:bigint,size:bigint>,inputOtherFiles:struct<numFiles:bigint,size:bigint>,inputNumZCubes:bigint,mergedFiles:struct<numFiles:bigint,size:bigint>,numOutputZCubes:bigint>,numBins:bigint,numBatches:bigint,totalConsideredFiles:bigint,totalFilesSkipped:bigint,preserveInsertionOrder:boolean,numFilesSkippedToReduceWriteAmplification:bigint,numBytesSkippedToReduceWriteAmplification:bigint,startTimeMs:bigint,endTimeMs:bigint,totalClusterParallelism:bigint,totalScheduledTasks:bigint,autoCompactParallelismStats:struct<maxClusterActiveParallelism:bigint,minClusterActiveParallelism:bigint,maxSessionActiveParallelism:bigint,minSessionActiveParallelism:bigint>,deletionVectorStats:struct<numDeletionVectorsRemoved:bigint,numDeletionVectorRowsRemoved:bigint>,numTableColumns:bigint,numTableColumnsWithStats:bigint>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_table.optimize().executeCompaction()\n",
    "\n",
    "#1min 56 secs 952mb file \n",
    "# with local 4 \n",
    "# 2min 9 sec 975 mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7f5efa69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n",
      "|version|           timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|   isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n",
      "|      4|2025-08-13 10:14:...|  NULL|    NULL| OPTIMIZE|{predicate -> [],...|NULL|    NULL|     NULL|          3|SnapshotIsolation|        false|{numRemovedFiles ...|        NULL|Apache-Spark/4.0....|\n",
      "|      3|2025-08-13 10:11:...|  NULL|    NULL|    WRITE|{mode -> Append, ...|NULL|    NULL|     NULL|          2|     Serializable|         true|{numFiles -> 4, n...|        NULL|Apache-Spark/4.0....|\n",
      "|      2|2025-08-13 10:11:...|  NULL|    NULL|    WRITE|{mode -> Append, ...|NULL|    NULL|     NULL|          1|     Serializable|         true|{numFiles -> 4, n...|        NULL|Apache-Spark/4.0....|\n",
      "|      1|2025-08-13 10:11:...|  NULL|    NULL|    WRITE|{mode -> Append, ...|NULL|    NULL|     NULL|          0|     Serializable|         true|{numFiles -> 4, n...|        NULL|Apache-Spark/4.0....|\n",
      "|      0|2025-08-13 10:10:...|  NULL|    NULL|    WRITE|{mode -> ErrorIfE...|NULL|    NULL|     NULL|       NULL|     Serializable|         true|{numFiles -> 4, n...|        NULL|Apache-Spark/4.0....|\n",
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delta_table.history().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a678b62",
   "metadata": {},
   "source": [
    "## **Z Order on id1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4ecf33ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_table = DeltaTable.forPath(spark, delta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dc4770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[path: string, metrics: struct<numFilesAdded:bigint,numFilesRemoved:bigint,filesAdded:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,filesRemoved:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,partitionsOptimized:bigint,zOrderStats:struct<strategyName:string,inputCubeFiles:struct<num:bigint,size:bigint>,inputOtherFiles:struct<num:bigint,size:bigint>,inputNumCubes:bigint,mergedFiles:struct<num:bigint,size:bigint>,numOutputCubes:bigint,mergedNumCubes:bigint>,clusteringStats:struct<inputZCubeFiles:struct<numFiles:bigint,size:bigint>,inputOtherFiles:struct<numFiles:bigint,size:bigint>,inputNumZCubes:bigint,mergedFiles:struct<numFiles:bigint,size:bigint>,numOutputZCubes:bigint>,numBins:bigint,numBatches:bigint,totalConsideredFiles:bigint,totalFilesSkipped:bigint,preserveInsertionOrder:boolean,numFilesSkippedToReduceWriteAmplification:bigint,numBytesSkippedToReduceWriteAmplification:bigint,startTimeMs:bigint,endTimeMs:bigint,totalClusterParallelism:bigint,totalScheduledTasks:bigint,autoCompactParallelismStats:struct<maxClusterActiveParallelism:bigint,minClusterActiveParallelism:bigint,maxSessionActiveParallelism:bigint,minSessionActiveParallelism:bigint>,deletionVectorStats:struct<numDeletionVectorsRemoved:bigint,numDeletionVectorRowsRemoved:bigint>,numTableColumns:bigint,numTableColumnsWithStats:bigint>]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_table.optimize().executeZOrderBy(\"id1\")\n",
    "# 3 min 975 mb 1 file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5a3d2ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n",
      "|version|           timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|   isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n",
      "|      5|2025-08-13 10:17:...|  NULL|    NULL| OPTIMIZE|{predicate -> [],...|NULL|    NULL|     NULL|          4|SnapshotIsolation|        false|{numRemovedFiles ...|        NULL|Apache-Spark/4.0....|\n",
      "|      4|2025-08-13 10:14:...|  NULL|    NULL| OPTIMIZE|{predicate -> [],...|NULL|    NULL|     NULL|          3|SnapshotIsolation|        false|{numRemovedFiles ...|        NULL|Apache-Spark/4.0....|\n",
      "|      3|2025-08-13 10:11:...|  NULL|    NULL|    WRITE|{mode -> Append, ...|NULL|    NULL|     NULL|          2|     Serializable|         true|{numFiles -> 4, n...|        NULL|Apache-Spark/4.0....|\n",
      "|      2|2025-08-13 10:11:...|  NULL|    NULL|    WRITE|{mode -> Append, ...|NULL|    NULL|     NULL|          1|     Serializable|         true|{numFiles -> 4, n...|        NULL|Apache-Spark/4.0....|\n",
      "|      1|2025-08-13 10:11:...|  NULL|    NULL|    WRITE|{mode -> Append, ...|NULL|    NULL|     NULL|          0|     Serializable|         true|{numFiles -> 4, n...|        NULL|Apache-Spark/4.0....|\n",
      "|      0|2025-08-13 10:10:...|  NULL|    NULL|    WRITE|{mode -> ErrorIfE...|NULL|    NULL|     NULL|       NULL|     Serializable|         true|{numFiles -> 4, n...|        NULL|Apache-Spark/4.0....|\n",
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delta_table.history().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc85675b",
   "metadata": {},
   "source": [
    "## **Z Order on id1 and id2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a89b66d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_table = DeltaTable.forPath(spark, delta_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f05cca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[path: string, metrics: struct<numFilesAdded:bigint,numFilesRemoved:bigint,filesAdded:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,filesRemoved:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,partitionsOptimized:bigint,zOrderStats:struct<strategyName:string,inputCubeFiles:struct<num:bigint,size:bigint>,inputOtherFiles:struct<num:bigint,size:bigint>,inputNumCubes:bigint,mergedFiles:struct<num:bigint,size:bigint>,numOutputCubes:bigint,mergedNumCubes:bigint>,clusteringStats:struct<inputZCubeFiles:struct<numFiles:bigint,size:bigint>,inputOtherFiles:struct<numFiles:bigint,size:bigint>,inputNumZCubes:bigint,mergedFiles:struct<numFiles:bigint,size:bigint>,numOutputZCubes:bigint>,numBins:bigint,numBatches:bigint,totalConsideredFiles:bigint,totalFilesSkipped:bigint,preserveInsertionOrder:boolean,numFilesSkippedToReduceWriteAmplification:bigint,numBytesSkippedToReduceWriteAmplification:bigint,startTimeMs:bigint,endTimeMs:bigint,totalClusterParallelism:bigint,totalScheduledTasks:bigint,autoCompactParallelismStats:struct<maxClusterActiveParallelism:bigint,minClusterActiveParallelism:bigint,maxSessionActiveParallelism:bigint,minSessionActiveParallelism:bigint>,deletionVectorStats:struct<numDeletionVectorsRemoved:bigint,numDeletionVectorRowsRemoved:bigint>,numTableColumns:bigint,numTableColumnsWithStats:bigint>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_table.optimize().executeZOrderBy(\"id1\", \"id2\")\n",
    "\n",
    "# 3min 6 sec 952mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e5c5f6e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n",
      "|version|           timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|   isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n",
      "|      6|2025-08-13 10:21:...|  NULL|    NULL| OPTIMIZE|{predicate -> [],...|NULL|    NULL|     NULL|          5|SnapshotIsolation|        false|{numRemovedFiles ...|        NULL|Apache-Spark/4.0....|\n",
      "|      5|2025-08-13 10:17:...|  NULL|    NULL| OPTIMIZE|{predicate -> [],...|NULL|    NULL|     NULL|          4|SnapshotIsolation|        false|{numRemovedFiles ...|        NULL|Apache-Spark/4.0....|\n",
      "|      4|2025-08-13 10:14:...|  NULL|    NULL| OPTIMIZE|{predicate -> [],...|NULL|    NULL|     NULL|          3|SnapshotIsolation|        false|{numRemovedFiles ...|        NULL|Apache-Spark/4.0....|\n",
      "|      3|2025-08-13 10:11:...|  NULL|    NULL|    WRITE|{mode -> Append, ...|NULL|    NULL|     NULL|          2|     Serializable|         true|{numFiles -> 4, n...|        NULL|Apache-Spark/4.0....|\n",
      "|      2|2025-08-13 10:11:...|  NULL|    NULL|    WRITE|{mode -> Append, ...|NULL|    NULL|     NULL|          1|     Serializable|         true|{numFiles -> 4, n...|        NULL|Apache-Spark/4.0....|\n",
      "|      1|2025-08-13 10:11:...|  NULL|    NULL|    WRITE|{mode -> Append, ...|NULL|    NULL|     NULL|          0|     Serializable|         true|{numFiles -> 4, n...|        NULL|Apache-Spark/4.0....|\n",
      "|      0|2025-08-13 10:10:...|  NULL|    NULL|    WRITE|{mode -> ErrorIfE...|NULL|    NULL|     NULL|       NULL|     Serializable|         true|{numFiles -> 4, n...|        NULL|Apache-Spark/4.0....|\n",
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delta_table.history().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87234c3",
   "metadata": {},
   "source": [
    "## **Cluster By - Liquid Clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8929047a",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_path_liquid_clustering = \".../data/liquid-test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6cd9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all parquet files in the folder\n",
    "df2 = (\n",
    "    spark.read.format(\"parquet\")\n",
    "    .option(\"header\", True)  # optional, Parquet stores schema internally\n",
    "    .load(r\"../data/data-1g\")  # <-- folder instead of single file\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a190fab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.createOrReplaceTempView(\"source_data_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "429b94bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.databricks.delta.clusteredTable.enableClusteringTablePreview\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073deb34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE liquid_clustered_table\n",
    "    USING DELTA\n",
    "    LOCATION '{delta_path_liquid_clustering}'\n",
    "    TBLPROPERTIES ('delta.columnMapping.mode' = 'name')\n",
    "    CLUSTER BY (id1)\n",
    "    AS SELECT * FROM source_data_view\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "#48secs 8files of 952mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2a7cc219",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_liquid_table = DeltaTable.forPath(spark, delta_path_liquid_clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8897e29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------+--------+--------------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|version|           timestamp|userId|userName|           operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+--------------------+------+--------+--------------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|      0|2025-08-13 10:29:...|  NULL|    NULL|CREATE TABLE AS S...|{partitionBy -> [...|NULL|    NULL|     NULL|       NULL|  Serializable|         true|{numFiles -> 8, n...|        NULL|Apache-Spark/4.0....|\n",
      "+-------+--------------------+------+--------+--------------------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delta_liquid_table.history().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589bca8c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e81766e6",
   "metadata": {},
   "source": [
    "## **Create views for all five versions of the Delta table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ba18346a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    spark.read.format(\"delta\")\n",
    "    .option(\"versionAsOf\", \"3\")\n",
    "    .load(delta_path)\n",
    "    .createOrReplaceTempView(\"x3\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bc2018db",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    spark.read.format(\"delta\")\n",
    "    .option(\"versionAsOf\", \"4\")\n",
    "    .load(delta_path)\n",
    "    .createOrReplaceTempView(\"x4\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ce9ce2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    spark.read.format(\"delta\")\n",
    "    .option(\"versionAsOf\", \"5\")\n",
    "    .load(delta_path)\n",
    "    .createOrReplaceTempView(\"x5\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "227b712e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    spark.read.format(\"delta\")\n",
    "    .option(\"versionAsOf\", \"6\")\n",
    "    .load(delta_path)\n",
    "    .createOrReplaceTempView(\"x6\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dca2ce7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for liquid clustering\n",
    "(\n",
    "    spark.read.format(\"delta\")\n",
    "    .option(\"versionAsOf\", \"0\")\n",
    "    .load(delta_path_liquid_clustering)\n",
    "    .createOrReplaceTempView(\"x7\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8f403995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deltalake \n",
    "import levi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62952182",
   "metadata": {},
   "source": [
    "## **query_a benchmarks(id1)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "44dd6564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 2.83 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(id1='id016', v1=796700.0)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "spark.sql(\n",
    "    \"select id1, sum(v1) as v1 from x3 where id1 = 'id016' group by id1\"\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "05fdd9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 31.2 ms\n",
      "Wall time: 2.02 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(id1='id016', v1=796700.0)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "spark.sql(\n",
    "    \"select id1, sum(v1) as v1 from x4 where id1 = 'id016' group by id1\"\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6204da8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 1.67 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(id1='id016', v1=796700.0)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "spark.sql(\n",
    "    \"select id1, sum(v1) as v1 from x5 where id1 = 'id016' group by id1\"\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8aa88ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 975 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(id1='id016', v1=796700.0)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "spark.sql(\n",
    "    \"select id1, sum(v1) as v1 from x6 where id1 = 'id016' group by id1\"\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b588745d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 31.2 ms\n",
      "Wall time: 1.01 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(id1='id016', v1=796700.0)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "spark.sql(\n",
    "    \"select id1, sum(v1) as v1 from x7 where id1 = 'id016' group by id1\"\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "604bf530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_files_<1mb': 0,\n",
       " 'num_files_1mb-500mb': 0,\n",
       " 'num_files_500mb-1gb': 1,\n",
       " 'num_files_1gb-2gb': 0,\n",
       " 'num_files_>2gb': 0}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = deltalake.DeltaTable(delta_path, version=6)\n",
    "levi.delta_file_sizes(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8a4cb57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 21.5 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'num_files': 1, 'num_files_skipped': 0, 'num_bytes_skipped': np.int64(0)}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "levi.skipped_stats(dt, filters=[(\"id1\", \"=\", \"'id016'\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47b4f8a",
   "metadata": {},
   "source": [
    "## **query_b benchmarks(id2)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fbca0ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 1.11 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(id2='id047', v1=798640.0)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "spark.sql(\n",
    "    \"select id2, sum(v1) as v1 from x3 where id2 = 'id047' group by id2\"\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fa1f6c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 31.2 ms\n",
      "Wall time: 1.09 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(id2='id047', v1=798640.0)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "spark.sql(\n",
    "    \"select id2, sum(v1) as v1 from x4 where id2 = 'id047' group by id2\"\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9f738d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 890 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(id2='id047', v1=798640.0)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "spark.sql(\n",
    "    \"select id2, sum(v1) as v1 from x5 where id2 = 'id047' group by id2\"\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2b43a355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 877 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(id2='id047', v1=798640.0)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "spark.sql(\n",
    "    \"select id2, sum(v1) as v1 from x6 where id2 = 'id047' group by id2\"\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4c537656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 1.12 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(id2='id047', v1=798640.0)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "spark.sql(\n",
    "    \"select id2, sum(v1) as v1 from x7 where id2 = 'id047' group by id2\"\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852836c4",
   "metadata": {},
   "source": [
    "## **query_c benchmarks(id1, id2)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "44451b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 1.34 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(id1='id016', id2='id047', sum(v1)=8152.0)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "spark.sql(\n",
    "    \"select id1, id2, sum(v1) from x3 where id1 = 'id016' and id2 = 'id047' group by id1, id2\"\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "85aed0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 1.39 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(id1='id016', id2='id047', sum(v1)=8152.0)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "spark.sql(\n",
    "    \"select id1, id2, sum(v1) from x4 where id1 = 'id016' and id2 = 'id047' group by id1, id2\"\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8dc61629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 31.2 ms\n",
      "Wall time: 1.19 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(id1='id016', id2='id047', sum(v1)=8152.0)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "spark.sql(\n",
    "    \"select id1, id2, sum(v1) from x5 where id1 = 'id016' and id2 = 'id047' group by id1, id2\"\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b5a1b2b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 31.2 ms\n",
      "Wall time: 1.02 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(id1='id016', id2='id047', sum(v1)=8152.0)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "spark.sql(\n",
    "    \"select id1, id2, sum(v1) from x6 where id1 = 'id016' and id2 = 'id047' group by id1, id2\"\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "45234c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 1.03 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(id1='id016', id2='id047', sum(v1)=8152.0)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "spark.sql(\n",
    "    \"select id1, id2, sum(v1) from x7 where id1 = 'id016' and id2 = 'id047' group by id1, id2\"\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f52971c",
   "metadata": {},
   "source": [
    "## **Getting to know about data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0c65e1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 548 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(v1=40000000)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "spark.sql(\"select count(*) as v1 from x4\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e2c73718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 4.32 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(id1='id089', v1=399920),\n",
       " Row(id1='id080', v1=399336),\n",
       " Row(id1='id087', v1=401884),\n",
       " Row(id1='id073', v1=397844),\n",
       " Row(id1='id064', v1=401416),\n",
       " Row(id1='id043', v1=400864),\n",
       " Row(id1='id051', v1=401332),\n",
       " Row(id1='id045', v1=400600),\n",
       " Row(id1='id074', v1=401584),\n",
       " Row(id1='id023', v1=399972),\n",
       " Row(id1='id006', v1=400156),\n",
       " Row(id1='id013', v1=400032),\n",
       " Row(id1='id099', v1=399716),\n",
       " Row(id1='id055', v1=398588),\n",
       " Row(id1='id052', v1=398132),\n",
       " Row(id1='id056', v1=402884),\n",
       " Row(id1='id093', v1=399936),\n",
       " Row(id1='id075', v1=401844),\n",
       " Row(id1='id034', v1=399048),\n",
       " Row(id1='id036', v1=400900),\n",
       " Row(id1='id032', v1=400992),\n",
       " Row(id1='id097', v1=401388),\n",
       " Row(id1='id059', v1=399456),\n",
       " Row(id1='id065', v1=400800),\n",
       " Row(id1='id005', v1=398900),\n",
       " Row(id1='id003', v1=398412),\n",
       " Row(id1='id037', v1=400176),\n",
       " Row(id1='id062', v1=399300),\n",
       " Row(id1='id094', v1=401076),\n",
       " Row(id1='id002', v1=399324),\n",
       " Row(id1='id090', v1=399088),\n",
       " Row(id1='id046', v1=398628),\n",
       " Row(id1='id030', v1=399516),\n",
       " Row(id1='id086', v1=399272),\n",
       " Row(id1='id063', v1=398520),\n",
       " Row(id1='id066', v1=395484),\n",
       " Row(id1='id057', v1=400132),\n",
       " Row(id1='id038', v1=401368),\n",
       " Row(id1='id021', v1=400472),\n",
       " Row(id1='id054', v1=398944),\n",
       " Row(id1='id033', v1=398024),\n",
       " Row(id1='id009', v1=400324),\n",
       " Row(id1='id042', v1=399136),\n",
       " Row(id1='id027', v1=399840),\n",
       " Row(id1='id069', v1=399104),\n",
       " Row(id1='id053', v1=399544),\n",
       " Row(id1='id028', v1=400136),\n",
       " Row(id1='id004', v1=399584),\n",
       " Row(id1='id084', v1=399012),\n",
       " Row(id1='id068', v1=401976),\n",
       " Row(id1='id058', v1=402340),\n",
       " Row(id1='id001', v1=400460),\n",
       " Row(id1='id079', v1=400788),\n",
       " Row(id1='id100', v1=400312),\n",
       " Row(id1='id083', v1=400264),\n",
       " Row(id1='id091', v1=398504),\n",
       " Row(id1='id007', v1=398160),\n",
       " Row(id1='id096', v1=402072),\n",
       " Row(id1='id072', v1=399700),\n",
       " Row(id1='id014', v1=398584),\n",
       " Row(id1='id085', v1=400800),\n",
       " Row(id1='id071', v1=402156),\n",
       " Row(id1='id060', v1=400252),\n",
       " Row(id1='id018', v1=401924),\n",
       " Row(id1='id041', v1=401344),\n",
       " Row(id1='id077', v1=400584),\n",
       " Row(id1='id020', v1=399032),\n",
       " Row(id1='id098', v1=397288),\n",
       " Row(id1='id008', v1=398772),\n",
       " Row(id1='id016', v1=398452),\n",
       " Row(id1='id039', v1=399768),\n",
       " Row(id1='id076', v1=400228),\n",
       " Row(id1='id025', v1=402616),\n",
       " Row(id1='id022', v1=401448),\n",
       " Row(id1='id082', v1=400840),\n",
       " Row(id1='id050', v1=400272),\n",
       " Row(id1='id047', v1=400860),\n",
       " Row(id1='id031', v1=401872),\n",
       " Row(id1='id010', v1=400328),\n",
       " Row(id1='id070', v1=399344),\n",
       " Row(id1='id044', v1=398156),\n",
       " Row(id1='id092', v1=399092),\n",
       " Row(id1='id026', v1=400204),\n",
       " Row(id1='id081', v1=399220),\n",
       " Row(id1='id095', v1=399720),\n",
       " Row(id1='id017', v1=399824),\n",
       " Row(id1='id048', v1=400868),\n",
       " Row(id1='id029', v1=400012),\n",
       " Row(id1='id049', v1=401064),\n",
       " Row(id1='id019', v1=400640),\n",
       " Row(id1='id035', v1=399608),\n",
       " Row(id1='id015', v1=400044),\n",
       " Row(id1='id012', v1=400520),\n",
       " Row(id1='id067', v1=399920),\n",
       " Row(id1='id040', v1=401456),\n",
       " Row(id1='id061', v1=400288),\n",
       " Row(id1='id011', v1=400388),\n",
       " Row(id1='id078', v1=396880),\n",
       " Row(id1='id088', v1=399244),\n",
       " Row(id1='id024', v1=399572)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "spark.sql(\"select id1, count(v1) as v1 from x4 group by id1\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b647d3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 2.65 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(v1=100)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "spark.sql(\"select count(distinct(id1)) as v1 from x4\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ea6c3ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 3.27 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(id2='id089', v1=397560),\n",
       " Row(id2='id080', v1=399100),\n",
       " Row(id2='id073', v1=401420),\n",
       " Row(id2='id087', v1=400460),\n",
       " Row(id2='id064', v1=398952),\n",
       " Row(id2='id043', v1=399020),\n",
       " Row(id2='id051', v1=398724),\n",
       " Row(id2='id045', v1=400964),\n",
       " Row(id2='id074', v1=400044),\n",
       " Row(id2='id023', v1=400232),\n",
       " Row(id2='id013', v1=400936),\n",
       " Row(id2='id006', v1=399532),\n",
       " Row(id2='id099', v1=398508),\n",
       " Row(id2='id055', v1=399712),\n",
       " Row(id2='id052', v1=401056),\n",
       " Row(id2='id056', v1=400940),\n",
       " Row(id2='id093', v1=400940),\n",
       " Row(id2='id034', v1=401076),\n",
       " Row(id2='id075', v1=398676),\n",
       " Row(id2='id036', v1=403380),\n",
       " Row(id2='id032', v1=398956),\n",
       " Row(id2='id097', v1=399092),\n",
       " Row(id2='id059', v1=397944),\n",
       " Row(id2='id065', v1=402292),\n",
       " Row(id2='id005', v1=400060),\n",
       " Row(id2='id003', v1=401180),\n",
       " Row(id2='id037', v1=399956),\n",
       " Row(id2='id062', v1=401524),\n",
       " Row(id2='id094', v1=397600),\n",
       " Row(id2='id002', v1=398060),\n",
       " Row(id2='id090', v1=399516),\n",
       " Row(id2='id046', v1=402476),\n",
       " Row(id2='id030', v1=401288),\n",
       " Row(id2='id086', v1=397340),\n",
       " Row(id2='id063', v1=401212),\n",
       " Row(id2='id066', v1=401068),\n",
       " Row(id2='id057', v1=400256),\n",
       " Row(id2='id038', v1=399472),\n",
       " Row(id2='id033', v1=400708),\n",
       " Row(id2='id021', v1=399256),\n",
       " Row(id2='id054', v1=397492),\n",
       " Row(id2='id009', v1=399992),\n",
       " Row(id2='id042', v1=399960),\n",
       " Row(id2='id027', v1=399268),\n",
       " Row(id2='id069', v1=402440),\n",
       " Row(id2='id028', v1=397744),\n",
       " Row(id2='id053', v1=401024),\n",
       " Row(id2='id004', v1=400600),\n",
       " Row(id2='id068', v1=400300),\n",
       " Row(id2='id084', v1=400940),\n",
       " Row(id2='id058', v1=400788),\n",
       " Row(id2='id001', v1=398932),\n",
       " Row(id2='id079', v1=400352),\n",
       " Row(id2='id100', v1=401056),\n",
       " Row(id2='id083', v1=400088),\n",
       " Row(id2='id007', v1=398632),\n",
       " Row(id2='id091', v1=398228),\n",
       " Row(id2='id072', v1=399212),\n",
       " Row(id2='id096', v1=399784),\n",
       " Row(id2='id014', v1=400456),\n",
       " Row(id2='id071', v1=399892),\n",
       " Row(id2='id085', v1=399728),\n",
       " Row(id2='id060', v1=400216),\n",
       " Row(id2='id018', v1=400104),\n",
       " Row(id2='id041', v1=400000),\n",
       " Row(id2='id077', v1=399640),\n",
       " Row(id2='id020', v1=403404),\n",
       " Row(id2='id098', v1=400812),\n",
       " Row(id2='id008', v1=399228),\n",
       " Row(id2='id016', v1=402204),\n",
       " Row(id2='id039', v1=401092),\n",
       " Row(id2='id076', v1=398704),\n",
       " Row(id2='id025', v1=399008),\n",
       " Row(id2='id022', v1=400320),\n",
       " Row(id2='id082', v1=399576),\n",
       " Row(id2='id050', v1=400820),\n",
       " Row(id2='id047', v1=399916),\n",
       " Row(id2='id031', v1=399940),\n",
       " Row(id2='id010', v1=399880),\n",
       " Row(id2='id070', v1=400308),\n",
       " Row(id2='id044', v1=398424),\n",
       " Row(id2='id092', v1=398592),\n",
       " Row(id2='id026', v1=400756),\n",
       " Row(id2='id081', v1=398760),\n",
       " Row(id2='id095', v1=398140),\n",
       " Row(id2='id017', v1=400264),\n",
       " Row(id2='id048', v1=399932),\n",
       " Row(id2='id029', v1=399124),\n",
       " Row(id2='id049', v1=402196),\n",
       " Row(id2='id035', v1=398516),\n",
       " Row(id2='id019', v1=399196),\n",
       " Row(id2='id015', v1=401692),\n",
       " Row(id2='id012', v1=399908),\n",
       " Row(id2='id067', v1=401416),\n",
       " Row(id2='id040', v1=399136),\n",
       " Row(id2='id011', v1=401636),\n",
       " Row(id2='id061', v1=400632),\n",
       " Row(id2='id078', v1=398704),\n",
       " Row(id2='id088', v1=399368),\n",
       " Row(id2='id024', v1=401040)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "spark.sql(\"select id2, count(v1) as v1 from x4 group by id2\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1cab8ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 31.2 ms\n",
      "Wall time: 1.96 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(v1=100)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "spark.sql(\"select count(distinct(id2)) as v1 from x4\").collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark4-clean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
